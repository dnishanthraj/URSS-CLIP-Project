# Zero-Shot Video Summariser using OpenAI CLIP

This project implements a **zero-shot, prompt-guided video summarisation pipeline** using [OpenAI CLIP](https://github.com/openai/CLIP) and complementary multimodal models.  
It was developed as part of the **University of Warwick URSS Research Scheme (2025)** to explore how pretrained vision language models can generate automatic highlights and textual summaries without task-specific training.

---

## Overview

The system converts any raw video into:
- **A concise highlight reel** (around 15 % of original length)
- **Automatic captions** via BLIP
- **Subtitles (.srt / .vtt)**  
- **Textual paragraph summary** generated by BART
- **Streamlit interface** for browsing, previewing, and analytics

It is designed to run end-to-end on the **Warwick Kudu (Gecko) GPU cluster**.

---

## Pipeline Summary

| Stage | Description |
|:------|:-------------|
| **1. Shot Sampling** | Split video into 2-second shots (OpenCV / PySceneDetect). |
| **2. CLIP Embedding** | Encode frames using `open_clip` ViT-B/32 model. |
| **3. Clustering** | Group visually similar shots with cosine similarity. |
| **4. Scoring** | Compare segment embeddings to text prompts (TVSum/News) for semantic relevance. |
| **5. Selection** | Apply knapsack optimisation to keep top 15 % of duration. |
| **6. Captioning** | BLIP generates captions for key frames. |
| **7. Highlight Generation** | Combine selected clips into a summary video with crossfades. |
| **8. Analytics + Streamlit UI** | Show coverage, redundancy, and segment browser. |

---

## Running on Kudu

1. SSH into the cluster:
   ```bash
   ssh kudu
   ```

2. Navigate to the project directory:

```bash
cd ~/urss/code
```

3. Submit a pipeline job for one video:

```bash
sbatch run_pipeline.sbatch XkqCExn6_Us.mp4
```

Logs are saved in `logs/`

Output summary files appear in `../work/<video_id>_fixed2s/`

4. Once completed, launch the Streamlit app locally or via port-forwarding:

```bash
streamlit run app.py
```

## Requirements

- **Python:** 3.10+
- **Libraries (core):**  
  `torch`, `open_clip_torch`, `transformers`, `moviepy`,  
  `scikit-learn`, `streamlit`, `pytesseract`, `opencv-python`,  
  `pandas`, `numpy`, `tqdm`, `pillow`, `scenedetect`
- **Hardware:** GPU strongly recommended for BLIP captioning and text summarisation.

Full environment specification is provided in **requirements.txt**, generated via `pip freeze`.  
To install all dependencies:

```bash
pip install -r requirements.txt

```

## Requirements
Evaluated on the TVSum benchmark (YFCC100M subset).
Prompt banks derived from TVSum category tags and news-related semantic queries.

## Citation / Acknowledgements

- OpenAI CLIP (Radford et al., 2021)
- Salesforce BLIP (Li et al., 2022)
- Facebook BART (Lewis et al., 2020)
- TVSum Dataset (Yale Song et al., CVPR 2015)