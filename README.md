# Zero-Shot Video Summariser using OpenAI CLIP

This project implements a **zero-shot, prompt-guided video summarisation pipeline** using [OpenAI CLIP](https://github.com/openai/CLIP) and complementary multimodal models.  
It was developed as part of the **University of Warwick URSS Research Scheme (2025)** to explore how pretrained visionlanguage models can generate automatic highlights and textual summaries without task-specific training.

---

## ?? Overview

The system converts any raw video into:
- **A concise highlight reel** (15 % of original length)
- **Automatic captions** via BLIP
- **Subtitles (.srt / .vtt)**  
- **Textual paragraph summary** generated by BART
- **Streamlit interface** for browsing, previewing, and analytics

It is designed to run end-to-end on the **Warwick Kudu (Gecko) GPU cluster**.

---

## ?? Pipeline Summary

| Stage | Description |
|:------|:-------------|
| **1. Shot Sampling** | Split video into 2-second shots (OpenCV / PySceneDetect). |
| **2. CLIP Embedding** | Encode frames using `open_clip` ViT-B/32 model. |
| **3. Clustering** | Group visually similar shots with cosine similarity. |
| **4. Scoring** | Compare segment embeddings to text prompts (TVSum/News) for semantic relevance. |
| **5. Selection** | Apply knapsack optimisation to keep top 15 % of duration. |
| **6. Captioning** | BLIP generates captions for key frames. |
| **7. Highlight Generation** | Combine selected clips into a summary video with crossfades. |
| **8. Analytics + Streamlit UI** | Show coverage, redundancy, and segment browser. |

---

## ?? Running on Kudu

1. SSH into the cluster:
   ```bash
   ssh yourusername@kudu.warwick.ac.uk
